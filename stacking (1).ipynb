{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":22,"outputs":[{"output_type":"stream","text":"/kaggle/input/home-credit-default-risk/application_test.csv\n/kaggle/input/home-credit-default-risk/installments_payments.csv\n/kaggle/input/home-credit-default-risk/application_train.csv\n/kaggle/input/home-credit-default-risk/HomeCredit_columns_description.csv\n/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\n/kaggle/input/home-credit-default-risk/sample_submission.csv\n/kaggle/input/home-credit-default-risk/bureau.csv\n/kaggle/input/home-credit-default-risk/credit_card_balance.csv\n/kaggle/input/home-credit-default-risk/previous_application.csv\n/kaggle/input/home-credit-default-risk/bureau_balance.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# загружаем данные\napp_train = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\napp_test = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\nsample_submission = pd.read_csv('/kaggle/input/home-credit-default-risk/sample_submission.csv')","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Шаг 1. Начнём. Притащим сюда модели из других моих ноутбуков"},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Логистическая регрессия\n\n**Препроцессинг:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"good_features = ['AMT_GOODS_PRICE', 'AMT_ANNUITY', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n                 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'DAYS_EMPLOYED', 'DAYS_BIRTH']\n\n# заполняем пропуски\nfeatures_list = []\n\nfor feature in good_features:\n    if (app_train[feature].isnull().sum() != 0):\n        features_list.append(feature)\nfor feature in features_list:\n    good_features.remove(feature)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features_list = ['AMT_GOODS_PRICE', 'AMT_ANNUITY', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n\nfor feature in features_list:\n    mean = app_train[feature].mean()\n    app_train[feature + '_filled'] = app_train[feature].fillna(mean)\n    \n    mean = app_test[feature].mean()\n    app_test[feature + '_filled'] = app_test[feature].fillna(mean)\n    feature = feature + '_filled'\n    good_features.append(feature)\n\n# логарифмируем признаки, чтобы привести к нормальному распределению\napp_train['AMT_INCOME_TOTAL_log'] = np.log1p(app_train['AMT_INCOME_TOTAL'])\napp_train = app_train.loc[app_train['AMT_INCOME_TOTAL_log'] < 14] #убираю выбросы\napp_test['AMT_INCOME_TOTAL_log'] = np.log1p(app_test['AMT_INCOME_TOTAL'])\n\napp_train['AMT_CREDIT_log'] = np.log1p(app_train.loc[:,['AMT_CREDIT']])\napp_test['AMT_CREDIT_log'] = np.log1p(app_test['AMT_CREDIT'])\n\napp_train['AMT_GOODS_PRICE_filled_log'] = np.log1p(app_train.loc[:,['AMT_GOODS_PRICE_filled']])\napp_test['AMT_GOODS_PRICE_filled_log'] = np.log1p(app_test['AMT_GOODS_PRICE_filled'])\n\napp_train['AMT_ANNUITY_filled_log'] = np.log1p(app_train.loc[:,['AMT_ANNUITY_filled']])\napp_test['AMT_ANNUITY_filled_log'] = np.log1p(app_test['AMT_ANNUITY_filled'])\n\nfor f in ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_GOODS_PRICE_filled', 'AMT_ANNUITY_filled']:\n    good_features.remove(f)\n    good_features.append(f + '_log')\n\n# Добавим ещё один признак, производный от первых двух: \n# TIME_TO_PAY = AMT_INCOME_TOTAL_log/AMT_CREDIT_log\napp_train['TIME_TO_PAY_log'] = app_train['AMT_INCOME_TOTAL_log']/app_train['AMT_CREDIT_log']\napp_test['TIME_TO_PAY_log'] = app_test['AMT_INCOME_TOTAL_log']/app_test['AMT_CREDIT_log']\n\ngood_features.append('TIME_TO_PAY_log')","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(good_features)","execution_count":27,"outputs":[{"output_type":"stream","text":"['DAYS_EMPLOYED', 'DAYS_BIRTH', 'EXT_SOURCE_1_filled', 'EXT_SOURCE_2_filled', 'EXT_SOURCE_3_filled', 'AMT_INCOME_TOTAL_log', 'AMT_CREDIT_log', 'AMT_GOODS_PRICE_filled_log', 'AMT_ANNUITY_filled_log', 'TIME_TO_PAY_log']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# нормируем признаки\nscaler = preprocessing.StandardScaler()\n\nto_norm = []\nfor f in good_features:\n    to_norm.append(f)\n\nfor feature in to_norm:\n    scaler.fit(app_train[[feature]])\n    app_train[[feature + '_norm']] = scaler.transform(app_train[[feature]])\n    app_test[[feature + '_norm']] = scaler.transform(app_test[[feature]])\n    good_features.remove(feature)\n    good_features.append(feature + '_norm')\nprint(good_features)","execution_count":28,"outputs":[{"output_type":"stream","text":"['DAYS_EMPLOYED_norm', 'DAYS_BIRTH_norm', 'EXT_SOURCE_1_filled_norm', 'EXT_SOURCE_2_filled_norm', 'EXT_SOURCE_3_filled_norm', 'AMT_INCOME_TOTAL_log_norm', 'AMT_CREDIT_log_norm', 'AMT_GOODS_PRICE_filled_log_norm', 'AMT_ANNUITY_filled_log_norm', 'TIME_TO_PAY_log_norm']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# делаем lable encoding для категориальных признаков\nfor col in ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n    le = preprocessing.LabelEncoder()\n    le.fit(app_train[col])\n    app_train[col] = le.transform(app_train[col])\n    app_test[col] = le.transform(app_test[col])\n\n# Encoding с One Hot Encoder\n#NAME_INCOME_TYPE + ORGANIZATION_TYPE\none_hot1 = pd.get_dummies(app_train['NAME_INCOME_TYPE'])\none_hot2 = pd.get_dummies(app_train['ORGANIZATION_TYPE'])\nnew_train = pd.concat([app_train,one_hot1,one_hot2], axis = 1)\n\none_hot1 = pd.get_dummies(app_test['NAME_INCOME_TYPE'])\none_hot2 = pd.get_dummies(app_test['ORGANIZATION_TYPE'])\nnew_test = pd.concat([app_test,one_hot1,one_hot2], axis = 1)\n\nnew_train, new_test = new_train.align(new_test, join = 'inner', axis = 1)\nnew_train['TARGET'] = app_train['TARGET']\n","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Сама модель лог-регрессии**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features3 = ['EXT_SOURCE_2_filled_norm',\n             'AMT_INCOME_TOTAL_log_norm','AMT_CREDIT_log_norm', 'TIME_TO_PAY_log_norm',\n             'AMT_GOODS_PRICE_filled_log']\nfeatures3 = good_features + list(one_hot1)\n#'DAYS_EMPLOYED', 'EXT_SOURCE_3_filled','EXT_SOURCE_1_filled','DAYS_BIRTH''AMT_ANNUITY_filled'","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_model = LogisticRegression(max_iter = 1000)\n\ncrosval = cross_val_score(log_model, new_train[features3],  y = new_train['TARGET'], cv = 5, scoring= 'roc_auc')\nprint(\"log_model:\",np.mean(crosval))\n\n# log_model: 0.6781624709571392 при max_iter = 1000","execution_count":32,"outputs":[{"output_type":"stream","text":"log_model: 0.7318468894761978\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 1.2 Случайный лес"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model_2 = RandomForestClassifier(random_state=2020, n_estimators = 50, max_depth = 4)\ncrosval = cross_val_score(forest_model_2, new_train[features3],  y = new_train['TARGET'], cv = 5, scoring= 'roc_auc')\nprint(\"forest_model_2:\",np.mean(crosval))\n#forest_model: 0.5964431371465481\n#forest_model_2: 0.6769968730270228\n#forest_model_2: 0.7088871309721058\n#forest_model_2: 0.7155592209105384 при features2 + features\n#forest_model_2: 0.7188116787406422\n#forest_model_2: 0.7201468633164378","execution_count":34,"outputs":[{"output_type":"stream","text":"forest_model_2: 0.7201454658968287\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Gradient boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_model_2 = LGBMClassifier(random_state=2020, n_estimators = 25, max_depth = 5, \n                             learning_rate = 0.1)\ncrosval = cross_val_score(boost_model_2, new_train[features3],  y = new_train['TARGET'], cv = 5, scoring= 'roc_auc')\nprint(\"boost_model_2:\",np.mean(crosval))\n#boost_model: 0.7307067571041472\n#boost_model: 0.7445376991869004","execution_count":43,"outputs":[{"output_type":"stream","text":"boost_model_2: 0.7354469211743099\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_model_3 = LGBMClassifier(random_state=2020, n_estimators = 50, max_depth = 4, \n                             learning_rate = 0.1)\ncrosval = cross_val_score(boost_model_3, new_train[features3],  y = new_train['TARGET'], cv = 5, scoring= 'roc_auc')\nprint(\"boost_model_3:\",np.mean(crosval))","execution_count":44,"outputs":[{"output_type":"stream","text":"boost_model_3: 0.7406190842761635\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 2. Реализуем stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stacking 1\nestimators = [('rf1', RandomForestClassifier(random_state=2020, n_estimators = 40, max_depth = 5)),\n              ('rf2', RandomForestClassifier(random_state=2035, n_estimators = 50, max_depth = 4)),\n              ('gb1', LGBMClassifier(random_state=2020, n_estimators = 25, max_depth = 5, learning_rate = 0.1)),\n              ('gb2', LGBMClassifier(random_state=2020, n_estimators = 50, max_depth = 4, learning_rate = 0.1)),\n              ('log', LogisticRegression(max_iter = 1000))]\n\nstacking_model = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression())\n\ncrosval = cross_val_score(stacking_model, new_train[features3],  y = new_train['TARGET'], cv = 5, scoring= 'roc_auc')\nprint(\"stacking_model1:\",np.mean(crosval))","execution_count":46,"outputs":[{"output_type":"stream","text":"stacking_model1: 0.7406002378062608\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stacking 2\nestimators = [('rf1', RandomForestClassifier(random_state=2020, n_estimators = 40, max_depth = 5)),\n              ('rf2', RandomForestClassifier(random_state=2035, n_estimators = 50, max_depth = 4)),\n              ('gb1', LGBMClassifier(random_state=2020, n_estimators = 25, max_depth = 5, learning_rate = 0.1)),\n              ('log', LogisticRegression(max_iter = 1000))]\n\nstacking_model = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression())\n\ncrosval = cross_val_score(stacking_model, new_train[features3],  y = new_train['TARGET'], cv = 5, scoring= 'roc_auc')\nprint(\"stacking_model2:\",np.mean(crosval))","execution_count":45,"outputs":[{"output_type":"stream","text":"boost_model: 0.7369046809259324\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"В случае, если убрать из stacking'а лучший алгоритм, результат заметно уменьшается на 0,003. В этот раз стэкинг показал результат лучший, чем лучший из составляющих его алгоритмов.\n\nРезультат ухудшился из-за того, что пропал алгоритм чаще других указывал верный результат и давал хорошие данные для обучения финальной модели"},{"metadata":{},"cell_type":"markdown","source":"P.S. Кажется, что можно было не делать отдельный ноутбук для стекинга, а сделать в том же, что и прошлый. В итоге оставил так, так как в этом ноутбуке неплохо получилось сократить кол-во кода для препроцессинга по сравнению с тем, что был для лог-регрессии"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}